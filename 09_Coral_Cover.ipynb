{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Coral Cover Analysis </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Import Necessary Libraries & load data\n",
    "\n",
    "The first step is to import the libraries you'll need. In this case, we're using Pandas to work with data and NumPy (although not used in this specific example). You can import these libraries as follows:\n",
    "\n",
    "**Note**: if you get an error: `ModuleNotFoundError: No module named 'x'` - add another cell and write `pip install x`.\n",
    "The reason for this error is that we are working with an online Python environment that is facilitated via Jupyter Notebooks, and there might be some packages that are not locally installed on your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import geopandas\n",
    "import geodatasets\n",
    "import contextily as cx\n",
    "import plotly.graph_objects as go\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.tri as tri\n",
    "import geopandas as gpd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data from a CSV File**\n",
    "\n",
    "To load data from a CSV file into a Pandas DataFrame, you can use the `pd.read_table()` function. This function is versatile and can handle various file formats. \n",
    "\n",
    "In this example, we'll use it to read data from `percent_cover_DFAT.csv` located in a subfolder called `data`. \n",
    "\n",
    "We assign the name `coral_data` to the DataFrame so we can reference it later in our analysis. You can choose any name you like, but make sure each dataset you load has a unique, descriptive name so you don’t overwrite existing data.\n",
    "\n",
    "Here's how you can do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_data=pd.read_csv('data/percent_cover_DFAT.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Formatting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Your Data Before Analysis\n",
    "\n",
    "When you first load a dataset, it’s important to spend time cleaning and formatting it before you begin plotting or running calculations. Raw data often comes with issues that can lead to errors or misleading results if left unchecked.\n",
    "\n",
    "1. **Check that the data has loaded correctly**\n",
    "- Use commands like df.head() to view the first few rows and df.info() to see column names, data types, and missing values.\n",
    "- Make sure the headings are recognised as column names, and not treated as data entries (sometimes CSV files contain extra header rows).\n",
    "\n",
    "2. **Inspect the indexing**\n",
    "- Every pandas DataFrame has an index (the row labels). By default, it starts from 0, 1, 2, etc.\n",
    "- Sometimes pandas may assign the first row of the data as an index, and if it isn't a true index this may cause problems later on when merging data or making plots.\n",
    "- When loading data you can specifiy you which column is the index or specify the data has no index `df = pd.read_csv(\"data.csv\", index_col=\"SiteID\")` or `df = pd.read_csv(\"data.csv\", index_col=False)`\n",
    "\n",
    "3. **Look at both the first and last rows**\n",
    "- The last row can sometimes contain “totals” or other automatic summary added by data collection software, which should not be analysed as real observations. Use df.tail() to check this.\n",
    "\n",
    "4. **Verify structure and consistency**\n",
    "- Confirm that numeric columns are stored as numbers (e.g., int64, float64), and catagory variables are stored as text (object).\n",
    "\n",
    "5. **Missing values**\n",
    "- Ensure missing values are correctly identified.\n",
    "- When a site has no species present, the data might be recorded as 0 (true zero) or left blank, which Python will interpret as NaN.\n",
    "- NaN values are treated as missing data, meaning they are ignored in calculations (like averages) and in plots.\n",
    "- It’s important to check your dataset and decide whether a value represents missing data or a true zero, and then code it appropriately.\n",
    "\n",
    "By taking these steps early, you set up a clean, consistent dataset that you can trust for plotting, summary statistics, and further analysis.\n",
    "\n",
    "Below are some typical commands that are useful for this initial step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coral_data.fillna(0, inplace=True)                            # Replace NaNs with 0\n",
    "#coral_data.head()                                             # preview first 5 rows\n",
    "#coral_data.info()                                             # summary of columns and data types\n",
    "#coral_data.describe()                                         # quick statistics\n",
    "#coral_data.isna().sum()                                       # check missing values\n",
    "#coral_data = coral_data.dropna()                              # drop rows with missing values\n",
    "#coral_data[\"col\"].fillna(0, inplace=True)                     # replace NaN with 0\n",
    "#coral_data.columns = coral_data.columns.str.strip()           # remove spaces in column names\n",
    "#coral_data = coral_data.rename(columns={\"old\": \"new\"})        # rename a column\n",
    "#coral_data = coral_data.drop(columns=[\"unnecessary\"])         # delete a column\n",
    "#coral_data = coral_data.drop(coral_data.index[-1])            # delete the last row\n",
    "#coral_data = coral_data.drop(coral_data.index[0])             # delete the first row\n",
    "#coral_data[\"HC\"] = coral_data[\"HC\"].astype(int)               # convert to integer\n",
    "#coral_data[\"Site\"] = coral_data[\"Site\"].astype(\"category\")    # convert to category\n",
    "#print(coral_data[\"Zone\"].unique())                            # print unique values\n",
    "#print(zone_data[\"Zone\"].value_counts())                       # print counts of each unique value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coralnet example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, inspect the dataset to confirm it loaded correctly and the structure makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "coral_data.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "# there are still 265 rows so there are no rows with missing values\n",
    "coral_data = coral_data.dropna()\n",
    "coral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our data we have:\n",
    "- **Rows:** each row is a single image.\n",
    "- **Columns:** each column is a benthic label.\n",
    "- **Values:** percent cover per image.\n",
    "- **Summary row:** a row at the end that calculates the average percent cover for each benthic label across all images. We want to delete this row.\n",
    "- **Image name:** These were assigned before uploading to CoralNet and follow the pattern `SiteID_Framenumber.png`. We want to split these into seperate columns for siteID number and frame number.\n",
    "- No missing values\n",
    "\n",
    "**Step 1 — delete the last row from the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python the last row or column can be indicated by '-1'\n",
    "coral_data = coral_data.drop(coral_data.index[-1])  \n",
    "coral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 — remove the `.png` extension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete text from file name \n",
    "coral_data['image name'] = coral_data['image name'].str.replace('.png', '', regex=False) ## to get rif of the .png \n",
    "coral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 — split `SiteID` and `Frame`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the values in the 'image name' column into parts wherever an underscore ('_') appears.\n",
    "# With expand=True, each part is placed into its own separate column (instead of a list in a single cell).\n",
    "# The result is stored in 'split_cols'\n",
    "split_cols = coral_data['image name'].str.split('_', expand=True) \n",
    "\n",
    "split_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we add the columns in split_columns back into our original DataFrame, which you will see are labelled as 0 and 1 by default\n",
    "coral_data = coral_data.join(split_cols)\n",
    "coral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to change the default column names to `Site` and `Frame`\n",
    "\n",
    "In pandas, this is done using the` .rename()` function. Column labels can be stored as either **integers** or **strings**, so it’s important to match the correct type when renaming:\n",
    "- If the column labels are integers (e.g., `0`, `1`), pass them as integers in the dictionary.\n",
    "- If the column labels are strings (e.g., `'0'`, `'1'`), pass them as strings (with quotes).\n",
    "\n",
    "If unsure, just try one and if you get an error, try the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "coral_data = coral_data.rename(columns={0: 'Site', 1: 'Frame'}) \n",
    "coral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4 — Add and merge data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you classify the underwater benthos at each site, you often want to add in **descripter** or **environmental variables** to aid in your analysis. \n",
    "\n",
    "For our example, we will add **latitude**, **longitude**, and **depth** for each **site**.\n",
    "\n",
    "We can do this by merging the coral net dataset with another CSV file that contains these details, using the site ID as the key for the merge.\n",
    "\n",
    "First, lets add in our `Metadata_dropcamera` file in the same way we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_data = pd.read_csv(\"data/Metadata_dropcamera.csv\")\n",
    "zone_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zone_data[\"Zone\"].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zone_data[\"Zone\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the ```merge()```function, specifying the two DataFrames (coral_data and zone_data) and the column to use as the key (Site).\n",
    "The how argument controls which rows are kept:\n",
    "\n",
    "- `left` → keeps all rows from the left DataFrame (data_coral) and only matching rows from the right.\n",
    "- `right` → keeps all rows from the right DataFrame (data_zone) and only matching rows from the left.\n",
    "- `inner` → keeps only the rows where the key exists in both DataFrames.\n",
    "- `outer` → keeps all rows from both DataFrames, filling missing values with NaN where no match exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform left merge to keep all rows from df2\n",
    "#data = pd.merge(coral_data, zone_data, on='Site', how='left')\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that we get an error when we try and run the above code. That is because our Site columns are saved as two differn't types object and intiger so they are treated as two differn't things. To fix this we can use the ```.dtypes```function to check what our columns are functioning as. It is best practice to check your variables at the beggining to avoid any errors. \n",
    "\n",
    "Typical data tpes include:\n",
    "- `int64` (Integer) : Whole numbers (no decimals).\n",
    "- `float64` (Floating Point): Numbers with decimals.\n",
    "- `object` (Generic Python Object, usually Text): Mixed or text/string data.\n",
    "- `catagory`: catagorical variable\n",
    "\n",
    "##### Option 1: \n",
    "\n",
    "Just code the function and swap the asterix so you can see each one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1, just prints the last data frame\n",
    "#zone_data.dtypes \n",
    "#coral_data.dtypes \n",
    "\n",
    "# option 2: prints both data frames\n",
    "print('zones:\\n',zone_data.dtypes,'\\n\\ncoralnet:\\n',coral_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the `print()` function**\n",
    "\n",
    "The `print()` function is useful for diaplaying multiple outputs at the same time with a mix of text and functions.\n",
    "Use `''` (or `\"\"`) around text strings.\n",
    "\n",
    "- Separate multiple items with a `,` → they will be printed with spaces between them.\n",
    "- Use `\\n` inside a string to create a new line (like pressing Enter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formatting variable types**\n",
    "\n",
    "Now we use the `.astype(`) function to change **site** in our DataFrame to **integer variables**. These values are listed as whole numbers, and even though these numbers don't mean anything in terms of order, it is easier to treat them as integers in this instance to make ensure reppeated numbers match each other.\n",
    "\n",
    "We will change **zone** to a catagorical variable. \n",
    "\n",
    "By default, categorical variables in pandas have **no inherent order** (e.g., categories like *low, medium, high* are treated equally). However, you can assign an order if it makes sense for your analysis.\n",
    "\n",
    "For example, we can assign an order to our **zone** variable, based on a spectrum of connectivity to the open ocean:\n",
    "\n",
    "- **Reef slope** → high connectivity\n",
    "- **Reef shelf** → medium connectivity\n",
    "- **Lagoons 1–3** → low connectivity\n",
    "\n",
    "Among the lagoons, **Lagoon 1** is the largest and most exposed to dominant wave energy, while **Lagoon 3** is the smallest and most sheltered. Therefore, we consider **Lagoon 3** to be the most isolated of all five zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to categorical\n",
    "zone_data[\"Site\"] = zone_data[\"Site\"].astype(\"int\")\n",
    "coral_data[\"Site\"] = coral_data[\"Site\"].astype(\"int\")\n",
    "coral_data[\"Frame\"] = coral_data[\"Frame\"].astype(\"category\")\n",
    "zone_data[\"Zone\"] = zone_data[\"Zone\"].astype(\"category\")\n",
    "\n",
    "# Set the category order\n",
    "zone_order = [\"Reef slope\", \"Reef flat\", \"Lagoon 1\", \"Lagoon 2\", \"Lagoon 3\"]\n",
    "zone_data[\"Zone\"] = zone_data[\"Zone\"].cat.set_categories(zone_order, ordered=True)\n",
    "\n",
    "#check it worked\n",
    "print('zones:\\n',zone_data.dtypes,'\\ncoralnet:\\n',coral_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our `merge.()` function will work because our **Site variable** matches across data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(coral_data, zone_data, on='Site', how='left')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export dataframe to a csv**\n",
    "\n",
    "We have finished formatting our data to be used for analysis. You might decide that you want to do analysis in another application, so here you would save your formatted data as a csv using the following code. \n",
    "\n",
    "`data.to_csv(\"formatted_data.csv\", index=False)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Plotting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many differnt plots we can use in python. \n",
    "\n",
    "These include:\n",
    "\n",
    "`.boxplot()` → shows the spread and distribution of data, highlighting the median, quartiles, and potential outliers.\n",
    "\n",
    "`.barplot()` → bar chart (mean values with error bars)\n",
    "\n",
    "`.pie()` → pie chart (proportions)\n",
    "\n",
    "`.scatterplot()` → scatter plot (relationship between two variables)\n",
    "\n",
    "`.lineplot()` → line graph (trend over time or sequence)\n",
    "\n",
    "`.hist()` → histogram (frequency of values)\n",
    "\n",
    "`.heatmap()` → colour-coded matrix (great for correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by looking at the mean reef scale changes in each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['HC']].boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `.iloc[:, 1:13]` to select all rows `(:)` and only columns 2 through 13 `(1:13)`. Remember that in Python indexing starts at 0, so column index 1 is actually the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:, 1:13].boxplot(figsize=(12, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the describe function to very quickly calculate common statistics, and then we can use these to make other plots based on the means ans dtandard deviations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = data.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat_coral = df_stat.T[0:12]   # transpose and select rows\n",
    "df_stat_coral[[\"mean\"]].plot.pie(\n",
    "    y=\"mean\",           # column to use for pie\n",
    "    legend=False,       # optional: hide legend (labels on wedges instead)\n",
    "    autopct='%1.1f%%',  # show percentages on slices\n",
    "    figsize=(6, 6)      # adjust figure size\n",
    ")\n",
    "\n",
    "plt.ylabel(\"\")  # remove the automatic 'mean' label on the y-axis\n",
    "plt.title(\"Mean values by column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat_coral = df_stat.T[0:12]   # transpose and select rows\n",
    "df_stat_coral[[\"mean\"]].plot.bar(\n",
    "    y=\"mean\",           # column to use for pie\n",
    "    legend=False,       # optional: hide legend (labels on wedges instead)\n",
    "    figsize=(6, 6)      # adjust figure size\n",
    ")\n",
    "\n",
    "plt.ylabel(\"\")  # remove the automatic 'mean' label on the y-axis\n",
    "plt.title(\"Mean values by column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by a variable such as \"reef zone\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the mean cover of each benthic class (e.g., hard coral, macroalgae, CCA, soft coral, etc.) across each reef zone.\n",
    "\n",
    "To do this in pandas, we can use the `groupby()` function:\n",
    "\n",
    "- `groupby(\"Zone\")` → groups the dataset by reef zone.\n",
    "- `.mean()` → calculates the mean for each numeric column within each group.\n",
    "- This way, we end up with a table where each row is a reef zone, and each column is the average percent cover of a benthic class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by reef zone and calculate the mean for each benthic class\n",
    "zone_means = data.groupby(\"Zone\").mean(numeric_only=True).iloc[:, 0:12]\n",
    "\n",
    "zone_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar chart with error bars\n",
    "zone_means[\"HC\"].plot.bar(figsize=(6,4))\n",
    "\n",
    "plt.title(\"Mean HC by Reef Zone\")\n",
    "plt.xlabel(\"Reef Zone\")\n",
    "plt.ylabel(\"Mean HC (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use SNS package to plot the mean and standard error from the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=data, x=\"Zone\", y=\"HC\", estimator=\"mean\", errorbar=\"se\")\n",
    "\n",
    "plt.title(\"Mean HC by Reef Zone\")\n",
    "plt.xlabel(\"Reef Zone\")\n",
    "plt.ylabel(\"Mean HC (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 3 Spatial distribution \n",
    "\n",
    "**Option 1: Overlaying data onto a map**\n",
    "\n",
    "Select specific columns ('GPS (S)', 'GPS (E)', and '% Cover Algae') from your coral_data DataFrame, \n",
    "\n",
    "The code then creates a scatter mapbox plot using Plotly Express `px.scatter_mapbox`. Here's a breakdown of the key parameters used:\n",
    "\n",
    "- **lat** and **lon**: Latitude and longitude columns from your DataFrame.\n",
    "- **size**: The size of the markers, determined by the % Hard Coral column.\n",
    "- **color**: The color of the markers, also determined by the % Hard Coral column.\n",
    "- **color_continuous_scale**: The color scale used for the markers.\n",
    "- **center**: The initial center of the map.\n",
    "- **opacity**: The opacity of the markers.\n",
    "- **zoom**: The initial zoom level of the map.\n",
    "- **mapbox_style**: The style of the map (in this case, **open-street-map**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(data, lat = 'lat', lon = 'long',\n",
    "                        color = 'HC', color_continuous_scale = 'rainbow',\n",
    "                        center = dict(lat = -23.498, lon = 152.07), opacity=0.7,\n",
    "                        zoom = 12, mapbox_style = 'open-street-map')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to save the image as a PNG image file with the dimensions (800x500 pixels).\n",
    "#fig.write_image(\"bubble-map-plotly.png\", width = 800, height = 500)\n",
    "\n",
    "#Or save as html to view in a browser:\n",
    "fig.write_html(\"bubble-map-plotly.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Overlaying a shapefile onto a typical scatterplot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numeric, forcing non-numeric entries to NaN\n",
    "data['lat'] = pd.to_numeric(data['lat'], errors='coerce')\n",
    "data['long'] = pd.to_numeric(data['long'], errors='coerce')\n",
    "\n",
    "print(data[['lat', 'long']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(data['long'], data['lat'])\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Sample locations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "Reef = gpd.read_file(\"data/OTR_DryReef.shp\")\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Background polygon\n",
    "Reef.plot(ax=ax, color=\"lightgrey\", edgecolor=\"black\", zorder=0) #zorder 0 means it will appear in the background\n",
    "# Points coloured by Hcoral\n",
    "ax.scatter(data['long'],\n",
    "           data['lat'],\n",
    "           c=data['HC'],\n",
    "           cmap=\"viridis\",\n",
    "           zorder=1 # zorder 1 means it will appear in the foreground\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of subplots**\n",
    "\n",
    "In Matplotlib, a figure `(fig)` is the overall container for plots, and an axes `(ax)` is a single subplot within that figure where the data is drawn. By using `fig, ax = plt.subplots()`, we create both the figure and one or more axes objects. Passing ax=ax into plotting functions (e.g., `Reef.plot(ax=ax, ...))` tells Python to draw the output on that specific subplot, instead of creating a new figure each time. This is useful for layering multiple datasets (such as reef polygons and survey points) in the same plot, or for arranging multiple subplots within one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "panel-cell-order": [
   "cd4cff1b",
   "f0d43b10",
   "2f69bd13",
   "b55c28e9",
   "b6b042ca",
   "549e0f1c",
   "595a7641",
   "eb8e3707",
   "87017ba2",
   "a9b2bd86",
   "d91b18a6",
   "3d60bce7",
   "4afd59e0",
   "e2dc5d95",
   "6a77b0eb",
   "57f9e9a5",
   "f6d7ac40",
   "96e1ccbe",
   "334762bf"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
